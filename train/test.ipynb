{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "from gym.wrappers import StepAPICompatibility\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "def fix(env, seed):\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "      torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env = StepAPICompatibility(env, new_step_api=True)\n",
    "fix(env, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.tanh(self.fc1(state))\n",
    "        hid = torch.tanh(hid)\n",
    "        return F.softmax(self.fc3(hid), dim=-1)\n",
    "\n",
    "# Value Network\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden = nn.Linear(8, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        value = self.output(outs)\n",
    "        return value\n",
    "    \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "class PolicyGradientAgent():\n",
    "\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.002)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=1000, gamma=0.9)\n",
    "        self.rewards = None\n",
    "        self.discounted_rewards = None\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def learn_A2C(self, log_probs, rewards, states, value_func):\n",
    "        with torch.no_grad():\n",
    "            values = value_func(states).squeeze()\n",
    "        advantages = rewards - values\n",
    "        loss = (-log_probs * advantages).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sample(self, state):\n",
    "        # Convert state to a FloatTensor\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).float()  # Convert directly from NumPy array to tensor\n",
    "        elif isinstance(state, list):\n",
    "            state = torch.FloatTensor(state)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected state format: {type(state)}\")\n",
    "\n",
    "        # Get action probabilities and sample an action\n",
    "        action_prob = self.network(state)\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "    # def sample(self, state):\n",
    "    #     action_prob = self.network(torch.FloatTensor(state))\n",
    "    #     action_dist = Categorical(action_prob)\n",
    "    #     action = action_dist.sample()\n",
    "    #     log_prob = action_dist.log_prob(action)\n",
    "    #     return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -101.9, Final: -100.0:   0%|          | 1/500 [00:00<02:33,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (437,)\n",
      "Log probs size:  torch.Size([437])\n",
      "Rewards size:  torch.Size([437])\n",
      "AVG Total Reward so far: -101.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -373.0, Final: -100.0:   0%|          | 2/500 [00:00<02:31,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (543,)\n",
      "Log probs size:  torch.Size([543])\n",
      "Rewards size:  torch.Size([543])\n",
      "AVG Total Reward so far: -372.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -179.3, Final: -100.0:   1%|          | 3/500 [00:00<02:36,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (507,)\n",
      "Log probs size:  torch.Size([507])\n",
      "Rewards size:  torch.Size([507])\n",
      "AVG Total Reward so far: -179.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -161.4, Final: -100.0:   1%|          | 4/500 [00:01<02:14,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (365,)\n",
      "Log probs size:  torch.Size([365])\n",
      "Rewards size:  torch.Size([365])\n",
      "AVG Total Reward so far: -161.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -136.6, Final: -100.0:   1%|          | 5/500 [00:01<02:15,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (437,)\n",
      "Log probs size:  torch.Size([437])\n",
      "Rewards size:  torch.Size([437])\n",
      "AVG Total Reward so far: -136.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -142.8, Final: -100.0:   1%|          | 6/500 [00:01<02:13,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (490,)\n",
      "Log probs size:  torch.Size([490])\n",
      "Rewards size:  torch.Size([490])\n",
      "AVG Total Reward so far: -142.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -197.7, Final: -100.0:   1%|▏         | 7/500 [00:01<02:08,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (446,)\n",
      "Log probs size:  torch.Size([446])\n",
      "Rewards size:  torch.Size([446])\n",
      "AVG Total Reward so far: -197.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -134.8, Final: -100.0:   2%|▏         | 8/500 [00:02<02:02,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (416,)\n",
      "Log probs size:  torch.Size([416])\n",
      "Rewards size:  torch.Size([416])\n",
      "AVG Total Reward so far: -134.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -288.9, Final: -100.0:   2%|▏         | 9/500 [00:02<02:11,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (585,)\n",
      "Log probs size:  torch.Size([585])\n",
      "Rewards size:  torch.Size([585])\n",
      "AVG Total Reward so far: -288.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -234.6, Final: -100.0:   2%|▏         | 10/500 [00:02<02:08,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (464,)\n",
      "Log probs size:  torch.Size([464])\n",
      "Rewards size:  torch.Size([464])\n",
      "AVG Total Reward so far: -234.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -172.3, Final: -100.0:   2%|▏         | 11/500 [00:02<02:11,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (480,)\n",
      "Log probs size:  torch.Size([480])\n",
      "Rewards size:  torch.Size([480])\n",
      "AVG Total Reward so far: -172.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -148.4, Final: -100.0:   2%|▏         | 12/500 [00:03<02:11,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (509,)\n",
      "Log probs size:  torch.Size([509])\n",
      "Rewards size:  torch.Size([509])\n",
      "AVG Total Reward so far: -148.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -154.0, Final: -100.0:   3%|▎         | 13/500 [00:03<02:07,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (459,)\n",
      "Log probs size:  torch.Size([459])\n",
      "Rewards size:  torch.Size([459])\n",
      "AVG Total Reward so far: -153.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -183.2, Final: -100.0:   3%|▎         | 14/500 [00:03<02:02,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (426,)\n",
      "Log probs size:  torch.Size([426])\n",
      "Rewards size:  torch.Size([426])\n",
      "AVG Total Reward so far: -183.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -198.0, Final: -100.0:   3%|▎         | 15/500 [00:03<02:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (455,)\n",
      "Log probs size:  torch.Size([455])\n",
      "Rewards size:  torch.Size([455])\n",
      "AVG Total Reward so far: -198.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -222.1, Final: -100.0:   3%|▎         | 16/500 [00:04<02:02,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (482,)\n",
      "Log probs size:  torch.Size([482])\n",
      "Rewards size:  torch.Size([482])\n",
      "AVG Total Reward so far: -222.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -164.7, Final: -100.0:   3%|▎         | 17/500 [00:04<02:03,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (487,)\n",
      "Log probs size:  torch.Size([487])\n",
      "Rewards size:  torch.Size([487])\n",
      "AVG Total Reward so far: -164.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -109.6, Final: -100.0:   4%|▎         | 18/500 [00:04<01:58,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (421,)\n",
      "Log probs size:  torch.Size([421])\n",
      "Rewards size:  torch.Size([421])\n",
      "AVG Total Reward so far: -109.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -97.4, Final: -100.0:   4%|▍         | 19/500 [00:04<02:00,  3.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (455,)\n",
      "Log probs size:  torch.Size([455])\n",
      "Rewards size:  torch.Size([455])\n",
      "AVG Total Reward so far: -97.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -210.9, Final: -100.0:   4%|▍         | 20/500 [00:05<01:58,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (432,)\n",
      "Log probs size:  torch.Size([432])\n",
      "Rewards size:  torch.Size([432])\n",
      "AVG Total Reward so far: -210.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -241.0, Final: -100.0:   4%|▍         | 21/500 [00:05<01:57,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (462,)\n",
      "Log probs size:  torch.Size([462])\n",
      "Rewards size:  torch.Size([462])\n",
      "AVG Total Reward so far: -241.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -178.0, Final: -100.0:   4%|▍         | 22/500 [00:05<02:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (532,)\n",
      "Log probs size:  torch.Size([532])\n",
      "Rewards size:  torch.Size([532])\n",
      "AVG Total Reward so far: -177.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -119.8, Final: -100.0:   5%|▍         | 23/500 [00:06<02:03,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (530,)\n",
      "Log probs size:  torch.Size([530])\n",
      "Rewards size:  torch.Size([530])\n",
      "AVG Total Reward so far: -119.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -150.3, Final: -100.0:   5%|▍         | 24/500 [00:06<02:01,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (465,)\n",
      "Log probs size:  torch.Size([465])\n",
      "Rewards size:  torch.Size([465])\n",
      "AVG Total Reward so far: -150.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -149.3, Final: -100.0:   5%|▌         | 25/500 [00:06<02:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (470,)\n",
      "Log probs size:  torch.Size([470])\n",
      "Rewards size:  torch.Size([470])\n",
      "AVG Total Reward so far: -149.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -115.4, Final: -100.0:   5%|▌         | 26/500 [00:06<02:12,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (561,)\n",
      "Log probs size:  torch.Size([561])\n",
      "Rewards size:  torch.Size([561])\n",
      "AVG Total Reward so far: -115.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -122.2, Final: -100.0:   5%|▌         | 27/500 [00:07<02:09,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (478,)\n",
      "Log probs size:  torch.Size([478])\n",
      "Rewards size:  torch.Size([478])\n",
      "AVG Total Reward so far: -122.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -84.5, Final: -100.0:   6%|▌         | 28/500 [00:07<02:26,  3.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (513,)\n",
      "Log probs size:  torch.Size([513])\n",
      "Rewards size:  torch.Size([513])\n",
      "AVG Total Reward so far: -84.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -170.6, Final: -100.0:   6%|▌         | 29/500 [00:07<02:24,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (439,)\n",
      "Log probs size:  torch.Size([439])\n",
      "Rewards size:  torch.Size([439])\n",
      "AVG Total Reward so far: -170.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -133.5, Final: -100.0:   6%|▌         | 30/500 [00:08<02:23,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (556,)\n",
      "Log probs size:  torch.Size([556])\n",
      "Rewards size:  torch.Size([556])\n",
      "AVG Total Reward so far: -133.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -238.6, Final: -100.0:   6%|▌         | 31/500 [00:08<02:27,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (613,)\n",
      "Log probs size:  torch.Size([613])\n",
      "Rewards size:  torch.Size([613])\n",
      "AVG Total Reward so far: -238.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -221.5, Final: -100.0:   6%|▋         | 32/500 [00:08<02:21,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (515,)\n",
      "Log probs size:  torch.Size([515])\n",
      "Rewards size:  torch.Size([515])\n",
      "AVG Total Reward so far: -221.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -149.3, Final: -100.0:   7%|▋         | 33/500 [00:09<02:22,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (508,)\n",
      "Log probs size:  torch.Size([508])\n",
      "Rewards size:  torch.Size([508])\n",
      "AVG Total Reward so far: -149.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -139.5, Final: -100.0:   7%|▋         | 34/500 [00:09<02:17,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (496,)\n",
      "Log probs size:  torch.Size([496])\n",
      "Rewards size:  torch.Size([496])\n",
      "AVG Total Reward so far: -139.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -162.8, Final: -100.0:   7%|▋         | 35/500 [00:09<02:12,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (476,)\n",
      "Log probs size:  torch.Size([476])\n",
      "Rewards size:  torch.Size([476])\n",
      "AVG Total Reward so far: -162.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -177.8, Final: -100.0:   7%|▋         | 36/500 [00:09<02:05,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (449,)\n",
      "Log probs size:  torch.Size([449])\n",
      "Rewards size:  torch.Size([449])\n",
      "AVG Total Reward so far: -177.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -78.5, Final: -80.7:   7%|▋         | 36/500 [00:10<02:05,  3.69it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (1390,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -143.2, Final: -100.0:   7%|▋         | 37/500 [00:11<04:25,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs size:  torch.Size([1390])\n",
      "Rewards size:  torch.Size([1390])\n",
      "AVG Total Reward so far: -78.53\n",
      "Rewards shape: (497,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -143.8, Final: -100.0:   8%|▊         | 38/500 [00:11<03:43,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs size:  torch.Size([497])\n",
      "Rewards size:  torch.Size([497])\n",
      "AVG Total Reward so far: -143.22\n",
      "Rewards shape: (478,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -143.8, Final: -100.0:   8%|▊         | 39/500 [00:11<03:11,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs size:  torch.Size([478])\n",
      "Rewards size:  torch.Size([478])\n",
      "AVG Total Reward so far: -143.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -191.6, Final: -100.0:   8%|▊         | 40/500 [00:11<02:59,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (560,)\n",
      "Log probs size:  torch.Size([560])\n",
      "Rewards size:  torch.Size([560])\n",
      "AVG Total Reward so far: -191.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -167.3, Final: -100.0:   8%|▊         | 41/500 [00:12<02:43,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (515,)\n",
      "Log probs size:  torch.Size([515])\n",
      "Rewards size:  torch.Size([515])\n",
      "AVG Total Reward so far: -167.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -136.3, Final: -100.0:   8%|▊         | 42/500 [00:12<02:31,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (508,)\n",
      "Log probs size:  torch.Size([508])\n",
      "Rewards size:  torch.Size([508])\n",
      "AVG Total Reward so far: -136.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -199.6, Final: -100.0:   9%|▊         | 43/500 [00:12<02:23,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (506,)\n",
      "Log probs size:  torch.Size([506])\n",
      "Rewards size:  torch.Size([506])\n",
      "AVG Total Reward so far: -199.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -100.6, Final: -100.0:   9%|▉         | 44/500 [00:13<02:13,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (449,)\n",
      "Log probs size:  torch.Size([449])\n",
      "Rewards size:  torch.Size([449])\n",
      "AVG Total Reward so far: -100.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -77.1, Final: -100.0:   9%|▉         | 45/500 [00:13<02:07,  3.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (447,)\n",
      "Log probs size:  torch.Size([447])\n",
      "Rewards size:  torch.Size([447])\n",
      "AVG Total Reward so far: -77.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -100.1, Final: -100.0:   9%|▉         | 46/500 [00:13<02:05,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (509,)\n",
      "Log probs size:  torch.Size([509])\n",
      "Rewards size:  torch.Size([509])\n",
      "AVG Total Reward so far: -100.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -93.2, Final: -100.0:   9%|▉         | 47/500 [00:13<02:08,  3.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (480,)\n",
      "Log probs size:  torch.Size([480])\n",
      "Rewards size:  torch.Size([480])\n",
      "AVG Total Reward so far: -93.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -61.1, Final: -80.0:   9%|▉         | 47/500 [00:15<02:08,  3.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (1486,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -180.9, Final: -100.0:  10%|▉         | 48/500 [00:15<05:09,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs size:  torch.Size([1486])\n",
      "Rewards size:  torch.Size([1486])\n",
      "AVG Total Reward so far: -61.13\n",
      "Rewards shape: (449,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -180.9, Final: -100.0:  10%|▉         | 49/500 [00:15<04:14,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probs size:  torch.Size([449])\n",
      "Rewards size:  torch.Size([449])\n",
      "AVG Total Reward so far: -180.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -149.5, Final: -100.0:  10%|█         | 50/500 [00:16<03:40,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (561,)\n",
      "Log probs size:  torch.Size([561])\n",
      "Rewards size:  torch.Size([561])\n",
      "AVG Total Reward so far: -149.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -102.3, Final: -100.0:  10%|█         | 51/500 [00:16<03:09,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (487,)\n",
      "Log probs size:  torch.Size([487])\n",
      "Rewards size:  torch.Size([487])\n",
      "AVG Total Reward so far: -102.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -88.6, Final: -100.0:  10%|█         | 52/500 [00:16<03:05,  2.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (700,)\n",
      "Log probs size:  torch.Size([700])\n",
      "Rewards size:  torch.Size([700])\n",
      "AVG Total Reward so far: -88.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -201.9, Final: -100.0:  11%|█         | 53/500 [00:17<03:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (594,)\n",
      "Log probs size:  torch.Size([594])\n",
      "Rewards size:  torch.Size([594])\n",
      "AVG Total Reward so far: -201.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -184.1, Final: -100.0:  11%|█         | 54/500 [00:17<03:12,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (840,)\n",
      "Log probs size:  torch.Size([840])\n",
      "Rewards size:  torch.Size([840])\n",
      "AVG Total Reward so far: -184.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -96.0, Final: -100.0:  11%|█         | 55/500 [00:17<03:02,  2.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (562,)\n",
      "Log probs size:  torch.Size([562])\n",
      "Rewards size:  torch.Size([562])\n",
      "AVG Total Reward so far: -96.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -212.8, Final: -100.0:  11%|█         | 56/500 [00:18<03:04,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (741,)\n",
      "Log probs size:  torch.Size([741])\n",
      "Rewards size:  torch.Size([741])\n",
      "AVG Total Reward so far: -212.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -191.4, Final: -100.0:  11%|█▏        | 57/500 [00:18<02:57,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (657,)\n",
      "Log probs size:  torch.Size([657])\n",
      "Rewards size:  torch.Size([657])\n",
      "AVG Total Reward so far: -191.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -139.7, Final: -100.0:  12%|█▏        | 58/500 [00:19<02:45,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (576,)\n",
      "Log probs size:  torch.Size([576])\n",
      "Rewards size:  torch.Size([576])\n",
      "AVG Total Reward so far: -139.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -181.5, Final: -100.0:  12%|█▏        | 59/500 [00:19<02:53,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (732,)\n",
      "Log probs size:  torch.Size([732])\n",
      "Rewards size:  torch.Size([732])\n",
      "AVG Total Reward so far: -181.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -255.7, Final: -100.0:  12%|█▏        | 60/500 [00:19<02:53,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (694,)\n",
      "Log probs size:  torch.Size([694])\n",
      "Rewards size:  torch.Size([694])\n",
      "AVG Total Reward so far: -255.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -145.4, Final: -100.0:  12%|█▏        | 61/500 [00:20<02:53,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (704,)\n",
      "Log probs size:  torch.Size([704])\n",
      "Rewards size:  torch.Size([704])\n",
      "AVG Total Reward so far: -145.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -169.6, Final: -100.0:  12%|█▏        | 62/500 [00:20<02:45,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (553,)\n",
      "Log probs size:  torch.Size([553])\n",
      "Rewards size:  torch.Size([553])\n",
      "AVG Total Reward so far: -169.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -259.7, Final: -100.0:  13%|█▎        | 63/500 [00:21<02:49,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (739,)\n",
      "Log probs size:  torch.Size([739])\n",
      "Rewards size:  torch.Size([739])\n",
      "AVG Total Reward so far: -259.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -148.0, Final: -100.0:  13%|█▎        | 64/500 [00:21<02:36,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (531,)\n",
      "Log probs size:  torch.Size([531])\n",
      "Rewards size:  torch.Size([531])\n",
      "AVG Total Reward so far: -147.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -108.5, Final: -100.0:  13%|█▎        | 65/500 [00:21<02:51,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (839,)\n",
      "Log probs size:  torch.Size([839])\n",
      "Rewards size:  torch.Size([839])\n",
      "AVG Total Reward so far: -108.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -86.0, Final: -100.0:  13%|█▎        | 66/500 [00:22<02:40,  2.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (566,)\n",
      "Log probs size:  torch.Size([566])\n",
      "Rewards size:  torch.Size([566])\n",
      "AVG Total Reward so far: -86.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -102.5, Final: -100.0:  13%|█▎        | 67/500 [00:22<02:27,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (486,)\n",
      "Log probs size:  torch.Size([486])\n",
      "Rewards size:  torch.Size([486])\n",
      "AVG Total Reward so far: -102.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -102.1, Final: -100.0:  14%|█▎        | 68/500 [00:22<02:31,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (659,)\n",
      "Log probs size:  torch.Size([659])\n",
      "Rewards size:  torch.Size([659])\n",
      "AVG Total Reward so far: -102.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -169.0, Final: -100.0:  14%|█▍        | 69/500 [00:23<02:32,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (527,)\n",
      "Log probs size:  torch.Size([527])\n",
      "Rewards size:  torch.Size([527])\n",
      "AVG Total Reward so far: -168.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -273.3, Final: -100.0:  14%|█▍        | 70/500 [00:23<02:43,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (766,)\n",
      "Log probs size:  torch.Size([766])\n",
      "Rewards size:  torch.Size([766])\n",
      "AVG Total Reward so far: -273.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total: -161.0, Final: -100.0:  14%|█▍        | 70/500 [00:23<02:26,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards shape: (580,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/4lm4s_yd2dl9yhpjcj2r47pw0000gn/T/ipykernel_46452/2137215592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize the rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Print shapes for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k9/4lm4s_yd2dl9yhpjcj2r47pw0000gn/T/ipykernel_46452/3025289728.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, log_probs, rewards)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "agent.network.train()  # Switch network into training mode\n",
    "EPISODE_PER_BATCH = 5  # update the agent every 5 episodes\n",
    "NUM_BATCH = 500        # totally update the agent for 500 episodes\n",
    "gamma = 0.99            # Discount factor\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # Update the training loop to extract the state correctly\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        result = env.reset()\n",
    "        if isinstance(result, tuple):\n",
    "            state, _ = result  # If reset returns a tuple, extract the state\n",
    "        else:\n",
    "            state = result  # If it returns only the state\n",
    "\n",
    "        total_reward, total_step = 0, 0\n",
    "        episode_rewards = []  # Store episode-specific rewards\n",
    "\n",
    "        while True:\n",
    "            action, log_prob = agent.sample(state)  # Get action and log probability\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob)  # Store log probability\n",
    "            episode_rewards.append(reward)  # Store immediate reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "\n",
    "            if done or truncated:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "\n",
    "        # Convert episode rewards to discounted cumulative rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        # Compute cumulative decaying rewards for the episode\n",
    "        for r in reversed(episode_rewards):\n",
    "            cumulative_reward = r + gamma * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)  # Insert at the front to keep the right order\n",
    "\n",
    "        rewards.extend(discounted_rewards)  # Extend the rewards list with the episode's discounted rewards\n",
    "\n",
    "    print(f\"Rewards shape: {np.shape(rewards)}\")\n",
    "\n",
    "    # Record training process\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # Update the agent using cumulative decaying rewards\n",
    "    rewards = np.array(rewards)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # Normalize the rewards\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "\n",
    "    # Print shapes for debugging\n",
    "    print(\"Log probs size: \", torch.stack(log_probs).size())\n",
    "    print(\"Rewards size: \", torch.from_numpy(rewards).size())\n",
    "\n",
    "    # Print the AVG total reward achieved so far\n",
    "    print(f\"AVG Total Reward so far: {avg_total_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01233473,  1.3889593 , -0.41315836, -0.31314465,  0.01414089,\n",
       "         0.09668748,  0.        ,  0.        ], dtype=float32),\n",
       " 0.35363735044923034,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected state format: <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k9/4lm4s_yd2dl9yhpjcj2r47pw0000gn/T/ipykernel_46452/1979479988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get action and log probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k9/4lm4s_yd2dl9yhpjcj2r47pw0000gn/T/ipykernel_46452/2061591770.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state format: {type(state)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Get action probabilities and sample an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected state format: <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "\n",
    "agent.network.train()  # Switch network into training mode\n",
    "EPISODE_PER_BATCH = 5  # update the agent every 5 episodes\n",
    "NUM_BATCH = 500        # totally update the agent for 500 episodes\n",
    "gamma = 0.99           # Discount factor\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # Collect trajectory\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        state = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "        episode_rewards = []  # Store episode-specific rewards\n",
    "\n",
    "        while True:\n",
    "            action, log_prob = agent.sample(state)  # Get action and log probability\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob)  # Store log probability\n",
    "            episode_rewards.append(reward)  # Store immediate reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "\n",
    "            if done or truncated:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "        # Convert episode rewards to discounted cumulative rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        # Compute cumulative decaying rewards for the episode\n",
    "        for r in reversed(episode_rewards):\n",
    "            cumulative_reward = r + gamma * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)  # Insert at the front to keep the right order\n",
    "\n",
    "        rewards.extend(discounted_rewards)  # Extend the rewards list with the episode's discounted rewards\n",
    "\n",
    "    # Record training process\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # Update the agent using cumulative decaying rewards\n",
    "    rewards = np.array(rewards)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # Normalize the rewards\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards).float())\n",
    "\n",
    "    # Print shapes less frequently to avoid slowing down training\n",
    "    if batch % 10 == 0:\n",
    "        print(f\"Batch {batch}: Log probs size: {torch.stack(log_probs).size()}, Rewards size: {torch.from_numpy(rewards).size()}\")\n",
    "        print(f\"AVG Total Reward so far: {avg_total_reward:.2f}\")\n",
    "\n",
    "plt.plot(avg_total_rewards)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Average Total Reward')\n",
    "plt.title('Policy Gradient Training Progress')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
